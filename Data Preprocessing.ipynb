{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit Data\n",
    "\n",
    "# Read the JSON file\n",
    "with open('data/project-4-at-2023-05-09-06-21-750d776a.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize empty lists for columns\n",
    "id_list = []\n",
    "body_list = []\n",
    "sentiment_list = []\n",
    "language_list = []\n",
    "vaccine_list = []\n",
    "date = []\n",
    "\n",
    "# Loop through the data and extract the required values\n",
    "for row in data:\n",
    "    sentiment = None\n",
    "    language = None\n",
    "    vaccine = None\n",
    "    for annotation in row['annotations'][0]['result']:\n",
    "        if annotation['from_name'] == 'sentiment':\n",
    "            sentiment = annotation['value']['choices'][0]\n",
    "        elif annotation['from_name'] == 'language':\n",
    "            language = annotation['value']['choices'][0]\n",
    "        elif annotation['from_name'] == 'vaccine':\n",
    "            vaccine = annotation['value']['choices'][0]\n",
    "    \n",
    "    # Check if any of the sentiment, language, or vaccine annotations are missing\n",
    "    if sentiment is None or language is None or vaccine is None:\n",
    "        continue\n",
    "    \n",
    "    id_list.append(row['id'])\n",
    "    body_list.append(row['data']['body'])\n",
    "    date.append(row['data']['created'])\n",
    "    sentiment_list.append(sentiment)\n",
    "    language_list.append(language)\n",
    "    vaccine_list.append(vaccine)\n",
    "\n",
    "# Create the DataFrame\n",
    "df1 = pd.DataFrame({\n",
    "    'id': id_list,\n",
    "    'tweet': body_list,\n",
    "    'sentiment': sentiment_list,\n",
    "    'language': language_list,\n",
    "    'vaccine': vaccine_list,\n",
    "    'date': date\n",
    "})\n",
    "\n",
    "df1['date'] = pd.to_datetime(df1['date'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file\n",
    "with open('data/project-3-at-2023-04-29-16-34-f2524c9c.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize empty lists for columns\n",
    "id_list = []\n",
    "body_list = []\n",
    "sentiment_list = []\n",
    "language_list = []\n",
    "vaccine_list = []\n",
    "date = []\n",
    "\n",
    "# Loop through the data and extract the required values\n",
    "for row in data:\n",
    "    sentiment = None\n",
    "    language = None\n",
    "    vaccine = None\n",
    "    for annotation in row['annotations'][0]['result']:\n",
    "        if annotation['from_name'] == 'sentiment':\n",
    "            sentiment = annotation['value']['choices'][0]\n",
    "        elif annotation['from_name'] == 'language':\n",
    "            language = annotation['value']['choices'][0]\n",
    "        elif annotation['from_name'] == 'vaccine':\n",
    "            vaccine = annotation['value']['choices'][0]\n",
    "    \n",
    "    # Check if any of the sentiment, language, or vaccine annotations are missing\n",
    "    if sentiment is None or language is None or vaccine is None:\n",
    "        continue\n",
    "    \n",
    "    id_list.append(row['data']['id'])\n",
    "    body_list.append(row['data']['Tweets'])\n",
    "    date.append(row['data']['date'])\n",
    "    sentiment_list.append(sentiment)\n",
    "    language_list.append(language)\n",
    "    vaccine_list.append(vaccine)\n",
    "\n",
    "# Create the DataFrame\n",
    "df2 = pd.DataFrame({\n",
    "    'id': id_list,\n",
    "    'tweet': body_list,\n",
    "    'sentiment': sentiment_list,\n",
    "    'language': language_list,\n",
    "    'vaccine': vaccine_list,\n",
    "    'date' : date\n",
    "})\n",
    "\n",
    "df2['date'] = pd.to_datetime(df2['date'], format='%Y-%m-%d %H:%M:%S+00:00').dt.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8451, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add source column\n",
    "df1['source'] = 'Reddit'\n",
    "df2['source'] = 'Twitter'\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Remove duplicate tweets\n",
    "df = df.drop_duplicates(subset=['tweet'])\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the tweets\n",
    "df['tweet'] = df['tweet'].str.lower()\n",
    "# remove numbers\n",
    "df['tweet'] = df['tweet'].str.replace(r'\\d+', '', regex=True)\n",
    "# remove punctuation\n",
    "df['tweet'] = df['tweet'].str.replace(r'[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [me, personally, ill, probably, be, taking, a,...\n",
       "1    [hello, im, one, of, the, people, whos, very, ...\n",
       "2    [hello, im, one, of, the, people, whos, very, ...\n",
       "3    [thanks, for, sharing, will, share, my, experi...\n",
       "4    [may, published, phase, kaso, the, thing, is, ...\n",
       "Name: tokenized_tweet, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "# Tokenize the tweets\n",
    "df['tokenized_tweet'] = df['tweet'].apply(tokenizer.tokenize)\n",
    "df['tokenized_tweet'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cebstemmer import stemmer\n",
    "from simplemma import text_lemmatizer\n",
    "from simplemma.langdetect import lang_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precompute the skip words\n",
    "SKIP_WORDS = set([\"vaccine\", \"covid-19\", \"moderna\", \"sinovac\", \"astrazeneca\", \"pfizer\", \"sputnik\", \"coronavirus\", \"covid\", \"covid19\", \"covid-19\", \"covidvaccine\", \"vaccines\", \"vaccination\", \"vaccinated\"])\n",
    "\n",
    "# dictionary to map language names to corresponding stemmer or lemmatizer\n",
    "STEMMERS = {\n",
    "    'Cebuano': stemmer.stem_word,\n",
    "    'Tagalog': lambda word: text_lemmatizer(word, lang='tl'),\n",
    "    'English': lambda word: text_lemmatizer(word, lang='en')\n",
    "}\n",
    "\n",
    "def stem_words_by_language(tok, lang, skip_words=SKIP_WORDS):\n",
    "    stemmer_func = STEMMERS.get(lang)\n",
    "    \n",
    "    if stemmer_func:\n",
    "        newtok = [stemmer_func(i) if i not in skip_words else i for i in tok]\n",
    "    else:\n",
    "        # if language is not detected, use list comprehension instead of for loop\n",
    "        newtok = [text_lemmatizer(i, lang='tl') if i not in skip_words and lang_detector(i, lang=('tl', 'en'))[0][0] == 'tl' \n",
    "                  else text_lemmatizer(i, lang='en') if i not in skip_words and lang_detector(i, lang=('tl', 'en'))[0][0] == 'en' \n",
    "                  else i for i in tok]\n",
    "    \n",
    "    return newtok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words\n",
    "# df['stemmed_tweets'] =  df.apply(lambda x: stem_words_by_language(x['tokenized_tweet'], x['language']), axis=1)\n",
    "\n",
    "def fix_tokens(row):\n",
    "    fixed_tokens = []\n",
    "    for token in row:\n",
    "        if isinstance(token, list):\n",
    "            if len(token) > 0:\n",
    "                fixed_tokens.append(token[0])\n",
    "            else:\n",
    "                # skip empty lists\n",
    "                continue\n",
    "        else:\n",
    "            fixed_tokens.append(token)\n",
    "    return fixed_tokens\n",
    "\n",
    "# df['stemmed_tweets'] = df['stemmed_tweets'].apply(fix_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import emoji\n",
    "\n",
    "def remove_unwanted_tokens(row):\n",
    "    new_row = []\n",
    "    for token in row:\n",
    "        if token is not None:\n",
    "            if emoji.emoji_count(token) > 0:\n",
    "                new_row.append(token)\n",
    "            # if punctuation, skip\n",
    "            elif token in string.punctuation:\n",
    "                continue\n",
    "            # if single character, skip\n",
    "            elif len(token) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                new_row.append(token)\n",
    "    return new_row\n",
    "            \n",
    "\n",
    "# df['stemmed_tweets'] = df['stemmed_tweets'].apply(remove_unwanted_tokens)\n",
    "# df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    " \n",
    "Source: [Github link](https://github.com/eanunez/stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cebuano stopwords\n",
    "from stopwords import CEBUANO_STOP_WORDS, ENGLISH_STOP_WORDS, TAGALOG_STOP_WORDS\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(tokens, lang):\n",
    "    if lang == 'Cebuano':\n",
    "        stopwords = CEBUANO_STOP_WORDS |  ENGLISH_STOP_WORDS\n",
    "    elif lang == 'Tagalog':\n",
    "        stopwords = TAGALOG_STOP_WORDS | ENGLISH_STOP_WORDS\n",
    "    elif lang == 'English':\n",
    "        stopwords = ENGLISH_STOP_WORDS\n",
    "    elif lang == 'Taglish':\n",
    "        stopwords = TAGALOG_STOP_WORDS | ENGLISH_STOP_WORDS\n",
    "    else:\n",
    "        return tokens\n",
    "    \n",
    "    # add 'yung' to stopwords\n",
    "    stopwords = stopwords | frozenset(['yung', 'yun', 'lang'])\n",
    "\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['no_stopwords_tweet'] = df.apply(lambda x: remove_stopwords(x['stemmed_tweets'], x['language']), axis=1)\n",
    "\n",
    "df['no_stopwords_tweet'] = df.apply(lambda x: remove_stopwords(x['tokenized_tweet'], x['language']), axis=1)\n",
    "# Remove blank tokens\n",
    "def remove_blank_tokens(tokens):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token != '':\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens\n",
    "\n",
    "df['no_stopwords_tweet'] = df['no_stopwords_tweet'].apply(remove_blank_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows that contains only a single token in no_stopwords_tweet\n",
    "df = df[df['no_stopwords_tweet'].map(len) > 1]\n",
    "\n",
    "# convert list of words into sentences\n",
    "df['text'] = df['no_stopwords_tweet'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data\n",
    "df.to_csv('data/preprocessed_data.csv', index=False)\n",
    "\n",
    "# save as pickle\n",
    "df.to_pickle('data/preprocessed_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
